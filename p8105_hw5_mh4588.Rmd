---
title: "Assignment 5"
author: "Maggie Hsu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

set.seed(8105) #set seed
```
# Problem 1
```{r problem 1 - function}
#Let integers 1-365 represent dates from January 1st to December 31st (no leap days)
bday_matches <- function(n) { #where n is the number of people in the group
  birthdays <- sample(1:365,n,replace = TRUE)
  result = (sum(duplicated(birthdays)) > 0) #since true is interpreted as 1, if there is a duplicate (or true) present in the birthdays vector, the sum will be greater than 0
  result
} 
```


Next, run this function 10000 times for each group size between 2 and 50. For each group size, compute the probability that at least two people in the group will share a birthday by averaging across the 10000 simulation runs. 

```{r problem 1 - group simulation}
birthday_output = 
  expand_grid(
    group_size = c(2:50), #group sizes between 2-50
    iter = 1:10000 #iterate 10,000 times
  ) |> 
  mutate(
    birthday_output = map(group_size, bday_matches)
  ) |> 
  unnest(birthday_output)
```

```{r problem 1 - plot}
#Compute the probability that there are two people sharing a birthday
shared_prob <- birthday_output |> 
  group_by(group_size) |> 
  summarize(
    probability = mean(sum(birthday_output)/10000)
  ) 

#Plot the relationship between the number of people in a group and two people within the group sharing a birthday
shared_prob |> 
  ggplot(aes(x = group_size, y=probability)) + #the vec? 
  geom_point() +
  geom_smooth(color = "red")
                          
```
# Problem 2

```{r problem 2}
#Define function for simulation
t_test_sim = function(mu) {
  t_test_sim_out = vector("list", length = 5000) #initialize
  
  for (i in 1:5000){
  datasets = rnorm(n=30, mean = mu, sd=5) #Generate datasets
  
  t_test = t.test(datasets, mu=0, conf.level=0.95) |>
  broom::tidy() |> #t-test dataset against mu=0 at a 0.95 confidence level
    select(estimate, p.value)
  
  t_test_sim_out[[i]] <- tibble(
   mu=mu, t_test
  )
  }
  return(bind_rows(t_test_sim_out))
}

#repeat for mu=0 and 1-6
sim_results_df = vector("list", 7)
for (i in 0:6) {
  sim_results_df[[i+1]] <- t_test_sim(i)
}
sim_results = bind_rows(sim_results_df) #final data frame

```

```{r plot 1}
sim_results |>
  group_by(mu) |>
  summarize(
    power = sum(p.value<=0.05)/5000 #proportion of p-values less than or = 0.05 
  ) |>
  ggplot(aes(x=mu, y=power))+geom_point()+geom_line()+xlab("True Value of Mean mu")+ylab("Power")+labs(title="Relationship Between Statistical Power and True Mu Value") +  scale_x_continuous(breaks = seq(0, 6, by = 1)) #plot
```

```{r plot 2}
sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(estimate), #Average estimate of mu hat
    avg_mu_hat_rejected = mean(estimate[p.value<=0.05])
    ) |>
ggplot()+
  geom_line(aes(x=mu, y=avg_mu_hat, color="General Average Mu-Hat"))+
  geom_line(aes(x=mu, y=avg_mu_hat_rejected, color="Average Mu-Hat for Rejected Values"))+
  xlab("True Value of Mean mu")+
  ylab("Average Estimate of mu hat")+
  labs(title="Relationship Between Estimated and True Mu Value") + 
  scale_x_continuous(breaks = seq(0, 6, by = 1)) + 
  theme_classic()+ 
  scale_colour_manual(breaks = c("General Average Mu-Hat","Average Mu-Hat for Rejected Values"), values = c("red", "blue"))
```

# Problem 3
```{r problem 3}
homicide <- read_csv("./homicide-data.csv") #Import dataset
homicide =  mutate(city_state = paste(city, state, sep=', '), homicide) #establish city,state variable

homicide |>
  group_by(city_state) |>
  summarize(
    total_cases = n(),
    total_unsolved = sum(disposition==c("Closed without arrest","Open/No arrest"))
  ) 
  
```

```{r baltimore}
baltimore <- filter(homicide, city=="Baltimore") #Filter dataset to only show Baltimore homicides
baltimore_unsolved <- filter(baltimore, disposition==c("Closed without arrest","Open/No arrest"))

baltimore_prop <- prop.test(nrow(baltimore_unsolved),nrow(baltimore), p = (nrow(baltimore_unsolved)/nrow(baltimore)))

```