---
title: "Assignment 5"
author: "Maggie Hsu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

set.seed(8105) #set seed
```
# Problem 1
```{r problem 1 - function}
#Let integers 1-365 represent dates from January 1st to December 31st (no leap days)
bday_matches <- function(n) { #where n is the number of people in the group
  birthdays <- sample(1:365,n,replace = TRUE)
  result = (sum(duplicated(birthdays)) > 0) #since true is interpreted as 1, if there is a duplicate (or true) present in the birthdays vector, the sum will be greater than 0
  result
} 
```

```{r problem 1 - group simulation}
birthday_output = 
  expand_grid(
    group_size = c(2:50), #group sizes between 2-50
    iter = 1:10000 #iterate 10,000 times
  ) |> 
  mutate(
    birthday_output = map(group_size, bday_matches)
  ) |> 
  unnest(birthday_output)
```

```{r problem 1 - plot}
#Compute the probability that there are two people sharing a birthday
shared_prob <- birthday_output |> 
  group_by(group_size) |> 
  summarize(
    probability = mean(sum(birthday_output)/10000)
  ) 

#Plot the relationship between the number of people in a group and two people within the group sharing a birthday
shared_prob |> 
  ggplot(aes(x = group_size, y=probability)) + #the vec? 
  geom_point() +
  geom_smooth(color = "red")
                          
```
The probability of two people sharing a birthday increases as group number increases, but the rate of change decreases as group number increases. This probability approaches 1 when the group size is larger than 40, but does not reach 1. 

# Problem 2

```{r problem 2}
#Define function for simulation
t_test_sim = function(mu) {
  t_test_sim_out = vector("list", length = 5000) #initialize
  
  for (i in 1:5000){
  datasets = rnorm(n=30, mean = mu, sd=5) #Generate datasets
  
  t_test = t.test(datasets, mu=0, conf.level=0.95) |>
  broom::tidy() |> #t-test dataset against mu=0 at a 0.95 confidence level
    select(estimate, p.value)
  
  t_test_sim_out[[i]] <- tibble(
   mu=mu, t_test
  )
  }
  return(bind_rows(t_test_sim_out))
}

#repeat for mu=0 and 1-6
sim_results_df = vector("list", 7)
for (i in 0:6) {
  sim_results_df[[i+1]] <- t_test_sim(i)
}
sim_results = bind_rows(sim_results_df) #final data frame

```

```{r plot 1}
sim_results |>
  group_by(mu) |>
  summarize(
    power = sum(p.value<=0.05)/5000 #proportion of p-values less than or = 0.05 
  ) |>
  ggplot(aes(x=mu, y=power))+geom_point()+geom_line()+xlab("True Value of Mean mu")+ylab("Power")+labs(title="Relationship Between Statistical Power and True Mu Value") +  scale_x_continuous(breaks = seq(0, 6, by = 1)) #plot
```
Although power increases as mu increases, the amount of change is largest for the mean increasing from 1-2 and from 2-3. After mu=3, increases in mu have much less of an effect on power (and almost no effect from 4-5 and 5-6). 

```{r plot 2}
sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(estimate), #Average estimate of mu hat
    avg_mu_hat_rejected = mean(estimate[p.value<=0.05])
    ) |>
ggplot()+
  geom_line(aes(x=mu, y=avg_mu_hat, color="General Average Mu-Hat"))+
  geom_line(aes(x=mu, y=avg_mu_hat_rejected, color="Average Mu-Hat for Rejected Values"))+
  xlab("True Value of Mean mu")+
  ylab("Average Estimate of mu hat")+
  labs(title="Relationship Between Estimated and True Mu Value") + 
  scale_x_continuous(breaks = seq(0, 6, by = 1)) + 
  theme_classic()+ 
  scale_colour_manual(breaks = c("General Average Mu-Hat","Average Mu-Hat for Rejected Values"), values = c("red", "blue"))
```
The average mu-hat is generally higher for the rejected values as compared to the overall average (especially from mu=0,1,2,3), but the overall average more closely reflects the true mean value of mu. The sample averages are approximately equal to the true mean value at mu=4,5,and 6 for both the rejected and overall values. 

This could be because as mu increases, the power also increases (where there are more times the null is rejected) so the test is more likely to correctly reject the null hypothesis (therefore making the estimated mean of the samples closer to the actual mu-hat value)

# Problem 3
```{r problem 3}
homicide <- read_csv("./homicide-data.csv") #Import dataset

#tulsa, al observation should be tulsa, ok
which(homicide == "Tul-000769", arr.ind=TRUE) #find row number
homicide[50810, 9] <- "OK"

homicide =  mutate(city_state = paste(city, state, sep=', '), homicide) #establish city,state variable

unsolved = homicide |>
  group_by(city_state) |>
  summarize(
    total_cases = n(),
    total_unsolved = sum(disposition==c("Closed without arrest","Open/No arrest"))
  ) 
  unsolved
```
The raw data consists of `r nrow(homicide)` rows and `r ncol(homicide)`columns. Although the dataset initially says there are 51 distinct cities, one of the observations is mislabeled as "Tulsa,AL" instead of "Tulsa, OK" which had to be cleaned--there are actually 50 major cities within the dataset. The dataset describes homicides across these cities with the victim name and demographics, along with the date, location, and conclusion of the case. 

```{r baltimore}
baltimore <- filter(homicide, city=="Baltimore") #Filter dataset to only show Baltimore homicides
baltimore_unsolved <- filter(baltimore, disposition==c("Closed without arrest","Open/No arrest"))
```

```{r baltimore prop test}
baltimore_prop <- prop.test(nrow(baltimore_unsolved),nrow(baltimore), p = (nrow(baltimore_unsolved)/nrow(baltimore)))

 baltimore_prop |>
  broom::tidy() |> #pull estimate and confidence interval
    select(estimate, conf.low, conf.high)
```
```{r iterate cities, warning-FALSE}
prop_sim = function(index) {
  prop_out = vector("list", length = 4) #initialize 
  total = filter(homicide, city_state==index)
  unsolved_cases = filter(total, disposition==c("Closed without arrest","Open/No arrest")) 
  
  prop_test = prop.test(nrow(unsolved_cases),nrow(total),p = (nrow(unsolved_cases)/nrow(total))) |>
  broom::tidy() |> #run prop test and extract relevant values
  select(estimate, conf.low, conf.high)
  
  prop_out = tibble( #return output
   city=index, prop_test
  )
    return(prop_out)
  }
```

```{r iterate, warning=FALSE}
#iterate across cities
city_out = vector("list", 50) #initialize output

for (i in 1:nrow(unsolved)) {
  city = pull(unsolved, var = city_state)[i]
  city_out[[i]] <- prop_sim(city)
}

city_out = bind_rows(city_out)
```

```{r create plot}
city_out |>
  ggplot(aes(x=reorder(city,estimate),y=estimate))+ #organized by increasing proportions 
  geom_errorbar(aes(ymin=conf.low, ymax=conf.high))+
  geom_boxplot()+
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title="Estimated Proportion of Unsolved Homicides in Major Cities", x="City", y="Estimated Proportion of Unsolved Homicide Rate")
```

