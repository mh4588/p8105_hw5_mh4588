---
title: "Assignment 5"
author: "Maggie Hsu"
date: "`r Sys.Date()`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

set.seed(8105) #set seed
```
# Problem 1
```{r problem 1 - function}
#Let integers 1-365 represent dates from January 1st to December 31st (no leap days)
bday_matches <- function(n) { #where n is the number of people in the group
  birthdays <- sample(1:365,n,replace = TRUE)
  result = (sum(duplicated(birthdays)) > 0) #since true is interpreted as 1, if there is a duplicate (or true) present in the birthdays vector, the sum will be greater than 0
  result
} 
```

```{r problem 1 - group simulation}
birthday_output = 
  expand_grid(
    group_size = c(2:50), #group sizes between 2-50
    iter = 1:10000 #iterate 10,000 times
  ) |> 
  mutate(
    birthday_output = map(group_size, bday_matches)
  ) |> 
  unnest(birthday_output)
```

```{r problem 1 - plot}
#Compute the probability that there are two people sharing a birthday
shared_prob <- birthday_output |> 
  group_by(group_size) |> 
  summarize(
    probability = mean(sum(birthday_output)/10000)
  ) 

#Plot the relationship between the number of people in a group and two people within the group sharing a birthday
shared_prob |> 
  ggplot(aes(x = group_size, y=probability)) + #the vec? 
  geom_point() +
  geom_smooth(color = "red")
                          
```
The probability of two people sharing a birthday increases as group number increases, but the rate of change decreases as group number increases. This probability approaches 1 when the group size is larger than 40, but does not reach 1. 

# Problem 2

```{r problem 2}
#Define function for simulation
t_test_sim = function(mu) {
  t_test_sim_out = vector("list", length = 5000) #initialize
  
  for (i in 1:5000){
  datasets = rnorm(n=30, mean = mu, sd=5) #Generate datasets
  
  t_test = t.test(datasets, mu=0, conf.level=0.95) |>
  broom::tidy() |> #t-test dataset against mu=0 at a 0.95 confidence level
    select(estimate, p.value)
  
  t_test_sim_out[[i]] <- tibble(
   mu=mu, t_test
  )
  }
  return(bind_rows(t_test_sim_out))
}

#repeat for mu=0 and 1-6
sim_results_df = vector("list", 7)
for (i in 0:6) {
  sim_results_df[[i+1]] <- t_test_sim(i)
}
sim_results = bind_rows(sim_results_df) #final data frame

```

```{r plot 1}
sim_results |>
  group_by(mu) |>
  summarize(
    power = sum(p.value<=0.05)/5000 #proportion of p-values less than or = 0.05 
  ) |>
  ggplot(aes(x=mu, y=power))+geom_point()+geom_line()+xlab("True Value of Mean mu")+ylab("Power")+labs(title="Relationship Between Statistical Power and True Mu Value") +  scale_x_continuous(breaks = seq(0, 6, by = 1)) #plot
```
Although power increases as mu increases, the amount of change is largest for the mean increasing from 1-2 and from 2-3. After mu=3, increases in mu have much less of an effect on power (and almost no effect from 4-5 and 5-6). 

```{r plot 2}
sim_results |>
  group_by(mu) |>
  summarize(
    avg_mu_hat = mean(estimate), #Average estimate of mu hat
    avg_mu_hat_rejected = mean(estimate[p.value<=0.05])
    ) |>
ggplot()+
  geom_line(aes(x=mu, y=avg_mu_hat, color="General Average Mu-Hat"))+
  geom_line(aes(x=mu, y=avg_mu_hat_rejected, color="Average Mu-Hat for Rejected Values"))+
  xlab("True Value of Mean mu")+
  ylab("Average Estimate of mu hat")+
  labs(title="Relationship Between Estimated and True Mu Value") + 
  scale_x_continuous(breaks = seq(0, 6, by = 1)) + 
  theme_classic()+ 
  scale_colour_manual(breaks = c("General Average Mu-Hat","Average Mu-Hat for Rejected Values"), values = c("red", "blue"))
```
The average mu-hat is generally higher for the rejected values as compared to the overall average (especially from mu=0,1,2,3), but the overall average more closely reflects the true mean value of mu. The sample averages are approximately equal to the true mean value at mu=4,5,and 6 for both the rejected and overall values. 

This could be because as mu increases, the power also increases (where there are more times the null is rejected) so the test is more likely to correctly reject the null hypothesis (therefore making the estimated mean of the samples closer to the actual mu-hat value)

# Problem 3
```{r problem 3}
homicide <- read_csv("./homicide-data.csv") #Import dataset
homicide =  mutate(city_state = paste(city, state, sep=', '), homicide) #establish city,state variable

homicide |>
  group_by(city_state) |>
  summarize(
    total_cases = n(),
    total_unsolved = sum(disposition==c("Closed without arrest","Open/No arrest"))
  ) 
  
```

```{r baltimore}
baltimore <- filter(homicide, city=="Baltimore") #Filter dataset to only show Baltimore homicides
baltimore_unsolved <- filter(baltimore, disposition==c("Closed without arrest","Open/No arrest"))
```

```{r baltimore prop test}
baltimore_prop <- prop.test(nrow(baltimore_unsolved),nrow(baltimore), p = (nrow(baltimore_unsolved)/nrow(baltimore)))

 baltimore_prop |>
  broom::tidy() |> #pull estimate and confidence interval
    select(estimate, conf.low, conf.high)
```
```{r iterate cities}